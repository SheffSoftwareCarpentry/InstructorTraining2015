We must all watch one of the two videos below and submit a paragraph of feedback on the video they chose to watch as part of their application.

 * a: https://vimeo.com/139316669
 * b: https://vimeo.com/139181120

We submit these as part of the individual surveys but reproduce them here for the benefit of all.

## Mike's Feedback

FOO

## Jez's Feedback

I watched video 'b'. Overall a good, confident delivery and the instructor dealt well with questions from the audience. The instructor did spend quite a lot of time checking the lesson plan, but this will improve with more practice. The individual commands are explained well, but I feel this whole section would be easier to remember with a slight restructure:

1. State the intended task (find the file with the most lines), and make it a genuinely useful task (counting the lines in a `.pdb` file doesn't seem that useful)
2. Show the full pipeline to do this, execute it (to show it works) and talk through the high level logic (count the lines in each file, sort numerically, pick the last file in the list): the diagram in the lecture notes might be useful at this point
3. *Then* build up the pipeline one command at a time, explaining them in detail as you go
4. Show the full pipeline again to summarise
5. Give some specific additional task for people to try and achieve by modifying the basic pipeline if they get it working quickly

This structure would make the lesson flow better and provide a clearer recall structure to help learners remember the material. Redirection to a file could be introduced either at the start or at the end, but switching to pipes halfway through I felt broke up the flow a bit. Also, on a data management note: we could encourage good practice by showing people a `README` file or other documentation/metadata in the directory alongside the example data files.

## David's Feedback

FOO

## Anna's Feedback

FOO

## Drew's Feedback

FOO

## Will's Feedback

I watched video 'a'.  Delivering an interactive tutorial using Jupyter Notebooks is a great idea: code can be clearly distinguished from results and students can follow along, possibly experimenting if the session's moving too slow for them.  The instructor reacted very quickly and well to questions regarding array dimensionality, providing clarifying examples and asking questions to test comprehension.  Students were forthcoming with responses and other queries, an indication of good rapport in the room.  However, the instructor's questions only tested the comprehension of those that verbally responded; an online poll later in the session would help confirm that all are keeping up.  Common mistakes (e.g. Python using half-open ranges) were well highlighted.  Sessions were well sequenced so that ndarrays could be presented in terms of their similarities to semi-familiar concepts (lists).  I might have briefly mentioned similar datastructures in other high-level research computing languages and relevant differences (e.g. 0/1-based indexing): this would be of use if students were already somewhat familiar with e.g. Matlab or R and could help students translate ideas from the session to other languages.
